{\rtf1\ansi\ansicpg1252\cocoartf2822
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica;\f1\fswiss\fcharset0 Helvetica-Bold;\f2\fmodern\fcharset0 CourierNewPSMT;
\f3\fmodern\fcharset0 Courier;}
{\colortbl;\red255\green255\blue255;\red175\green0\blue219;\red163\green21\blue21;\red0\green128\blue0;
\red0\green0\blue255;\red121\green94\blue38;\red0\green16\blue128;\red37\green118\blue147;}
{\*\expandedcolortbl;;\csgenericrgb\c68627\c0\c85882;\csgenericrgb\c63922\c8235\c8235;\csgenericrgb\c0\c50196\c0;
\csgenericrgb\c0\c0\c100000;\csgenericrgb\c47451\c36863\c14902;\csgenericrgb\c0\c6275\c50196;\csgenericrgb\c14510\c46275\c57647;}
{\info
{\author Till Hilmar}}\paperw11900\paperh16840\margl1417\margr1417\margb1134\margt1417\vieww12200\viewh16740\viewkind1\viewscale134
\deftab708
\pard\pardeftab708\ri-6\partightenfactor0

\f0\fs24 \cf0 Python code documentation for the Manuscript "Signifying the Present in Links to the Past: Memory Organizations React to the February 24, 2022 Russian Full-Scale Invasion of Ukraine"\
\pard\pardeftab708\ri-6\partightenfactor0

\f1\b \cf0 \
# Preprocessing\
\
\pard\pardeftab708\ri-6\sl285\partightenfactor0

\f2\b0\fs21 \cf2 \expnd0\expndtw0\kerning0
from\cf0  google.colab \cf2 import\cf0  drive\
drive.mount(\cf3 '/content/drive'\cf0 )\
\
\cf2 import\cf0  pandas \cf2 as\cf0  pd\
\
\pard\pardeftab708\ri-6\sl285\partightenfactor0
\cf4 # Load the Excel file and specify the data type for the 'unique_tweet_id' column\cf0 \
file_path = \cf3 '/x.xlsx'\cf0 \
df = pd.read_excel(file_path)\
\pard\pardeftab708\ri-6\sl285\partightenfactor0
\cf5 !\cf0 python -m spacy download en_core_web_lg\
\
\pard\pardeftab708\ri-6\sl285\partightenfactor0
\cf4 # Import necessary libraries\cf0 \
\pard\pardeftab708\ri-6\sl285\partightenfactor0
\cf2 import\cf0  spacy\
\cf2 from\cf0  spacy.lang.en.stop_words \cf2 import\cf0  STOP_WORDS\
\cf2 import\cf0  re\
\
\pard\pardeftab708\ri-6\sl285\partightenfactor0
\cf4 # Load English tokenizer, tagger, parser, NER and word vectors\cf0 \
nlp = spacy.load(\cf3 "en_core_web_lg"\cf0 )\
\
\cf4 # Function to preprocess text\cf0 \
\pard\pardeftab708\ri-6\sl285\partightenfactor0
\cf5 def\cf0  \cf6 preprocess_text\cf0 (\cf7 text\cf0 ):\
    \cf4 # Create a spaCy object\cf0 \
    doc = nlp(text)\
\
    \cf4 # Lemmatization, lowercasing, removing stop words, urls, and symbols\cf0 \
    clean_text = \cf3 " "\cf0 .join(token.lemma_.lower() \cf2 for\cf0  token \cf5 in\cf0  doc\
                          \cf2 if\cf0  \cf5 not\cf0  token.is_stop\
                          \cf5 and\cf0  \cf5 not\cf0  token.like_url\
                          \cf5 and\cf0  \cf5 not\cf0  token.is_space\
                          \cf5 and\cf0  \cf5 not\cf0  token.orth_.isspace())\
\
    \cf4 # Remove any remaining URLs\cf0 \
    clean_text = re.sub(r\cf3 'http\\S+|www\\S+|https\\S+'\cf0 , \cf3 ''\cf0 , clean_text, flags=re.MULTILINE)\
\
    \cf2 return\cf0  clean_text\
\
\pard\pardeftab708\ri-6\sl285\partightenfactor0
\cf4 # Apply the preprocessing function to the 'text' column\cf0 \
df[\cf3 'clean_text'\cf0 ] = df[\cf3 'text'\cf0 ].apply(preprocess_text)\
df[\cf3 'clean_text'\cf0 ] = df[\cf3 'clean_text'\cf0 ].\cf8 str\cf0 .replace(\cf3 '_x000d_'\cf0 , \cf3 ''\cf0 )\
df[\cf3 'clean_text'\cf0 ] = df[\cf3 'clean_text'\cf0 ].\cf8 str\cf0 .replace(\cf3 '_ x000d _'\cf0 , \cf3 ''\cf0 )\
\
\cf4 # Save the dataframe to Excel\cf0 \
df.to_excel(\cf3 '/x.xlsx'\cf0 , index=\cf5 False\cf0 )\
\
\cf4 # Download the Excel file\cf0 \
\pard\pardeftab708\ri-6\sl285\partightenfactor0
\cf2 from\cf0  google.colab \cf2 import\cf0  files\
files.download(\cf3 '/x.xlsx'\cf0 )\
\
\pard\pardeftab708\ri-6\partightenfactor0

\f1\b\fs24 \cf0 \kerning1\expnd0\expndtw0 \
# First Model \'96 no finetuning applied\
\pard\pardeftab708\ri-6\partightenfactor0

\f0\b0 \cf0 \
\pard\pardeftab708\ri-6\sl285\partightenfactor0

\f2\fs21 \cf2 \expnd0\expndtw0\kerning0
from\cf0  google.colab \cf2 import\cf0  drive\
drive.mount(\cf3 '/content/drive'\cf0 )\
\
\cf2 import\cf0  pandas \cf2 as\cf0  pd\
\
\pard\pardeftab708\ri-6\sl285\partightenfactor0
\cf4 # Load the Excel file and specify the data type for the 'unique_tweet_id' column\cf0 \
file_path = \cf3 '/x.xlsx'\cf0 \
df = pd.read_excel(file_path)\
\
\pard\pardeftab708\ri-6\sl285\partightenfactor0
\cf5 !\cf0 python -m spacy download en_core_web_trf\
\
\cf5 !\cf0 pip install spacy-transformers\
\
\pard\pardeftab708\ri-6\sl285\partightenfactor0
\cf2 import\cf0  spacy\
\
\pard\pardeftab708\ri-6\sl285\partightenfactor0
\cf4 # Load SpaCy model for English\cf0 \
nlp_en = spacy.load(\cf3 'en_core_web_trf'\cf0 )\
\
\cf4 # Function to perform entity recognition and save results to DataFrame\cf0 \
\pard\pardeftab708\ri-6\sl285\partightenfactor0
\cf5 def\cf0  \cf6 perform_entity_recognition\cf0 (\cf7 df\cf0 , \cf7 nlp\cf0 ):\
    \cf4 # Initialize lists to store results\cf0 \
    dates = []\
    events = []\
    persons = []\
    norps = []  \cf4 # Nationalities or religious or political groups\cf0 \
    orgs = []\
    new_ids = []\
\
    \cf4 # Iterate over each row in the dataframe\cf0 \
    \cf2 for\cf0  index, row \cf5 in\cf0  df.iterrows():\
        \cf4 # Check if the text is missing, and if so, skip this row\cf0 \
        \cf2 if\cf0  pd.isnull(row[\cf3 'clean_text'\cf0 ]):\
            dates.append(\cf5 None\cf0 )\
            events.append(\cf5 None\cf0 )\
            persons.append(\cf5 None\cf0 )\
            norps.append(\cf5 None\cf0 )\
            orgs.append(\cf5 None\cf0 )\
            new_ids.append(row[\cf3 'new_id'\cf0 ])\
            \cf2 continue\cf0 \
\
        \cf4 # Perform entity recognition on the text\cf0 \
        doc = nlp(row[\cf3 'clean_text'\cf0 ])\
\
        \cf4 # Extract entities of interest (DATE, EVENT, PERSON, NORP, ORG)\cf0 \
        extracted_dates = []\
        extracted_events = []\
        extracted_persons = []\
        extracted_norps = []\
        extracted_orgs = []\
\
        \cf2 for\cf0  ent \cf5 in\cf0  doc.ents:\
            \cf2 if\cf0  ent.label_ == \cf3 'DATE'\cf0 :\
                extracted_dates.append(ent.text)\
            \cf2 elif\cf0  ent.label_ == \cf3 'EVENT'\cf0 :\
                extracted_events.append(ent.text)\
            \cf2 elif\cf0  ent.label_ == \cf3 'PERSON'\cf0 :\
                extracted_persons.append(ent.text)\
            \cf2 elif\cf0  ent.label_ == \cf3 'NORP'\cf0 :\
                extracted_norps.append(ent.text)\
            \cf2 elif\cf0  ent.label_ == \cf3 'ORG'\cf0 :\
                extracted_orgs.append(ent.text)\
\
        \cf4 # Append the extracted entities to the respective lists\cf0 \
        dates.append(extracted_dates)\
        events.append(extracted_events)\
        persons.append(extracted_persons)\
        norps.append(extracted_norps)\
        orgs.append(extracted_orgs)\
\
        \cf4 # Append other relevant information\cf0 \
        new_ids.append(row[\cf3 'new_id'\cf0 ])\
\
    \cf4 # Create a new dataframe to store the results\cf0 \
    result_df = pd.DataFrame(\{\
        \cf3 'new_id'\cf0 : new_ids,\
        \cf3 'date'\cf0 : dates,\
        \cf3 'event'\cf0 : events,\
        \cf3 'person'\cf0 : persons,\
        \cf3 'norp'\cf0 : norps,\
        \cf3 'org'\cf0 : orgs\
    \})\
\
    \cf2 return\cf0  result_df\
\
\pard\pardeftab708\ri-6\sl285\partightenfactor0
\cf4 # Call the function to perform entity recognition for the English language model\cf0 \
result_df = perform_entity_recognition(df, nlp_en)\
\
\cf4 # Save the dataframe to Excel\cf0 \
result_df.to_excel(\cf3 '/x.xlsx'\cf0 , index=\cf5 False\cf0 )\
\
\cf4 # Download the Excel file\cf0 \
\pard\pardeftab708\ri-6\sl285\partightenfactor0
\cf2 from\cf0  google.colab \cf2 import\cf0  files\
files.download(\cf3 '/x.xlsx'\cf0 )\
\
\pard\pardeftab708\ri-6\partightenfactor0

\f0\fs24 \cf0 \kerning1\expnd0\expndtw0 \
\pard\pardeftab708\ri-6\partightenfactor0

\f1\b \cf0 # Final Model \'96 Including Domain-Specific Fine-Tuning\
\pard\pardeftab708\ri-6\partightenfactor0

\f0\b0 \cf0 \
\pard\pardeftab708\ri-6\sl285\partightenfactor0

\f2\fs21 \cf2 \expnd0\expndtw0\kerning0
from\cf0  google.colab \cf2 import\cf0  drive\
drive.mount(\cf3 '/content/drive'\cf0 )\
\
\cf2 import\cf0  pandas \cf2 as\cf0  pd\
\
\pard\pardeftab708\ri-6\sl285\partightenfactor0
\cf4 # Load the Excel file and specify the data type for the 'unique_tweet_id' column\cf0 \
file_path = \cf3 '/x.xlsx'\cf0 \
df = pd.read_excel(file_path)\
\
\pard\pardeftab708\ri-6\sl285\partightenfactor0
\cf5 !\cf0 python -m spacy download en_core_web_trf\
\
\cf5 !\cf0 pip install spacy-transformers\
\
\pard\pardeftab708\ri-6\sl285\partightenfactor0
\cf2 import\cf0  spacy\
\
\pard\pardeftab708\ri-6\sl285\partightenfactor0
\cf4 # Load SpaCy model for English\cf0 \
nlp_en = spacy.load(\cf3 'en_core_web_trf'\cf0 )\
\pard\pardeftab708\ri-6\partightenfactor0

\f0\fs24 \cf0 \kerning1\expnd0\expndtw0 \
\pard\pardeftab708\ri-6\sl285\partightenfactor0

\f2\fs21 \cf2 \expnd0\expndtw0\kerning0
import\cf0  spacy\
\cf2 from\cf0  spacy.matcher \cf2 import\cf0  Matcher\
\
\pard\pardeftab708\ri-6\sl285\partightenfactor0
\cf4 # Load SpaCy model for English\cf0 \
nlp_en = spacy.load(\cf3 'en_core_web_trf'\cf0 )\
\
\cf4 # Initialize the matcher with the shared vocab\cf0 \
matcher = Matcher(nlp_en.vocab)\
\
\cf4 # List of events from R transformed into Python format\cf0 \
refined_NER_events = [\
    \cf3 "holodomor memorial day"\cf0 , \cf3 "holodomor"\cf0 , \cf3 "shoah"\cf0 , \cf3 "world war 2"\cf0 , \cf3 "world war ii"\cf0 ,\
    \cf3 "holocaust"\cf0 , \cf3 "corona pandemic"\cf0 , \cf3 "winter war"\cf0 , \cf3 "solidarity day"\cf0 , \cf3 "volyn massacre"\cf0 ,\
    \cf3 "independence day"\cf0 , \cf3 "polish army day"\cf0 , \cf3 "holodomor 1932 - 33"\cf0 , \cf3 "crimean tatar flag day"\cf0 ,\
    \cf3 "unity day"\cf0 , \cf3 "international vyshyvanka day"\cf0 , \cf3 "human rights day"\cf0 , \cf3 "2nd world war"\cf0 ,\
    \cf3 "second world war"\cf0 , \cf3 "european holocaust remembrance day"\cf0 , \cf3 "memory day"\cf0 , \cf3 "communist era"\cf0 ,\
    \cf3 "european day"\cf0 , \cf3 "internationalmuseumday"\cf0 , \cf3 "ukrainian revolution"\cf0 , \cf3 "soviet genocide"\cf0 ,\
    \cf3 "popular uprising"\cf0 , \cf3 "national day"\cf0 , \cf3 "anniversary holodomor"\cf0 , \cf3 "mass famine"\cf0 ,\
    \cf3 "srebrenica genocide"\cf0 , \cf3 "international children day"\cf0 , \cf3 "chernobyl"\cf0 , \cf3 "day dignity freedom"\cf0 ,\
    \cf3 "international holocaust remembrance day"\cf0 , \cf3 "november uprising"\cf0 , \cf3 "national flag day"\cf0 ,\
    \cf3 "national anthem day"\cf0 , \cf3 "koryukiv tragedy"\cf0 , \cf3 "war 2014"\cf0 , \cf3 "january uprising"\cf0 ,\
    \cf3 "brovary dnipro january 14th"\cf0 , \cf3 "orange revolution revolution dignity"\cf0 ,\
    \cf3 "international volunteer day"\cf0 , \cf3 "maidan - revolution"\cf0 , \cf3 "imperial war"\cf0 , \cf3 "ukrainian revolution"\cf0 ,\
    \cf3 "ukrainian cultural revival 1930"\cf0 , \cf3 "moscow invasion"\cf0 , \cf3 "armedforcesday"\cf0 , \cf3 "babyn yar tragedy"\cf0 ,\
    \cf3 "babyn yar"\cf0 , \cf3 "babyn yar massacre"\cf0 , \cf3 "german - soviet war"\cf0 , \cf3 "revolution dignity"\cf0 , \cf3 "soviet era"\cf0 ,\
    \cf3 "cold war"\cf0 , \cf3 "bolshevik offensive"\cf0 , \cf3 "red revolution"\cf0 , \cf3 "proskuriv offensive"\cf0 , \cf3 "june war"\cf0 ,\
    \cf3 "vyshyvanka day"\cf0 , \cf3 "stalin terror"\cf0 , \cf3 "eastern war"\cf0 , \cf3 "remembrance reconciliation day"\cf0 ,\
    \cf3 "winter campaign"\cf0 , \cf3 "prague spring"\cf0 , \cf3 "day crimean tatar flag"\cf0 , \cf3 "norilsk uprising"\cf0 ,\
    \cf3 "ribbentrop - molotov pact"\cf0 , \cf3 "71st anniversary victory nazism"\cf0 , \cf3 "maidan"\cf0 , \cf3 "katyn massacre"\cf0 ,\
    \cf3 "victory day"\cf0 , \cf3 "velvet revolution"\cf0 , \cf3 "october coup"\cf0 , \cf3 "holocaust remembrance day"\cf0 ,\
    \cf3 "constitution day"\cf0 , \cf3 "great famine 1932"\cf0 , \cf3 "ukraine day invasion"\cf0 , \cf3 "pereiaslav agreement"\cf0 ,\
    \cf3 "ww ii"\cf0 , \cf3 "national liberation struggle"\cf0 , \cf3 "polish - muscovite war"\cf0 , \cf3 "warsaw uprising"\cf0 ,\
    \cf3 "stalin war"\cf0 , \cf3 "feb invasion"\cf0 , \cf3 "barbarossa german - soviet war"\cf0 , \cf3 "bucha massacre"\cf0 ,\
    \cf3 "freedom day"\cf0 , \cf3 "occupation day"\cf0 , \cf3 "liberation ukraine"\cf0 , \cf3 "history war 1812"\cf0 ,\
    \cf3 "stalin death day"\cf0 , \cf3 "euromaidan"\cf0 , \cf3 "remembrance reconciliation day"\cf0 , \cf3 "budapest memorandum"\cf0 ,\
    \cf3 "red famine"\cf0 , \cf3 "1917 revolution"\cf0 , \cf3 "civil war"\cf0 , \cf3 "soviet invasion"\cf0 , \cf3 "warsaw pact"\cf0 ,\
    \cf3 "international workers' day"\cf0 , \cf3 "great terror"\cf0 , \cf3 "auschwitz"\cf0 , \cf3 "brest treaty"\cf0 , \cf3 "koryukiv tragedy"\cf0 ,\
    \cf3 "chornobyl"\cf0 , \cf3 "katyn"\cf0 , 
\f3 \cf3 "katy\uc0\u324 "
\f2 \cf0 , \cf3 "wolyn"\cf0 , \cf3 "volhynia"\cf0 , \cf3 "volyn"\cf0 , \cf3 "world war 1"\cf0 , \cf3 "world war i"\cf0 ,\
    \cf3 "ww1"\cf0 , \cf3 "first world war"\cf0 , \cf3 "1st world war"\cf0 , \cf3 "ww i"\cf0 \
]\
\
\cf4 # Function to convert events into spaCy patterns\cf0 \
\pard\pardeftab708\ri-6\sl285\partightenfactor0
\cf5 def\cf0  \cf6 create_pattern\cf0 (\cf7 event\cf0 ):\
    \cf2 return\cf0  [\{\cf3 'LOWER'\cf0 : token\} \cf2 for\cf0  token \cf5 in\cf0  event.split()]\
\
\pard\pardeftab708\ri-6\sl285\partightenfactor0
\cf4 # Create patterns for each event in the list\cf0 \
event_patterns = [create_pattern(event) \cf2 for\cf0  event \cf5 in\cf0  refined_NER_events]\
\
\cf4 # Add the new event patterns to the matcher\cf0 \
matcher.add(\cf3 'EVENT'\cf0 , patterns=event_patterns)\
\
\cf4 # New list of adjectives representing ethnicities and nationalities\cf0 \
refined_NER_adjectives = [\
    \cf3 "jewish"\cf0 , \cf3 "ukrainian"\cf0 , \cf3 "russian"\cf0 , \cf3 "nazi"\cf0 , \cf3 "roma"\cf0 , \cf3 "hungarian"\cf0 , \cf3 "german"\cf0 ,\
    \cf3 "israeli"\cf0 , \cf3 "non-white"\cf0 , \cf3 "belarusian"\cf0 , \cf3 "soviet"\cf0 , \cf3 "eastern european"\cf0 , \cf3 "polish"\cf0 ,\
    \cf3 "stalinist"\cf0 , \cf3 "latvian"\cf0 , \cf3 "american"\cf0 , \cf3 "british"\cf0 , \cf3 "lithuanian"\cf0 , \cf3 "bolshevik"\cf0 ,\
    \cf3 "moldovan"\cf0 , \cf3 "malaysian"\cf0 , \cf3 "french"\cf0 , \cf3 "austrian"\cf0 , \cf3 "czech"\cf0 , \cf3 "norwegian"\cf0 , \cf3 "kazakh"\cf0 ,\
    \cf3 "moldavian"\cf0 , \cf3 "sinti"\cf0 , \cf3 "chilean"\cf0 , \cf3 "venezuelan"\cf0 , \cf3 "romanian"\cf0 , \cf3 "brazilian"\cf0 ,\
    \cf3 "portuguese"\cf0 , \cf3 "kazakhstani"\cf0 , \cf3 "serbian"\cf0 , \cf3 "croatian"\cf0 , \cf3 "spanish"\cf0 , \cf3 "guatemalan"\cf0 ,\
    \cf3 "macedonian"\cf0 , \cf3 "swedish"\cf0 , \cf3 "australian"\cf0 , \cf3 "japanese"\cf0 , \cf3 "iranian"\cf0 , \cf3 "belgian"\cf0 , \cf3 "korean"\cf0 ,\
    \cf3 "baltic"\cf0 , \cf3 "bulgarian"\cf0 , \cf3 "armenian"\cf0 , \cf3 "indian"\cf0 , \cf3 "swiss"\cf0 , \cf3 "finnish"\cf0 , \cf3 "kurdish"\cf0 ,\
    \cf3 "turkish"\cf0 , \cf3 "british"\cf0 , \cf3 "yugoslav"\cf0 , \cf3 "romani"\cf0 , \cf3 "syrian"\cf0 , \cf3 "african"\cf0 , \cf3 "georgian"\cf0 ,\
    \cf3 "chinese"\cf0 , \cf3 "irish"\cf0 , \cf3 "dutch"\cf0 \
]\
\
\cf4 # Define the patterns for ethnic adjectives followed by a noun\cf0 \
ethnic_adjective_patterns = [[\{\cf3 'LOWER'\cf0 : adj\}, \{\cf3 'POS'\cf0 : \cf3 'NOUN'\cf0 \}] \cf2 for\cf0  adj \cf5 in\cf0  refined_NER_adjectives]\
\
\cf4 # Add the new patterns to the matcher\cf0 \
matcher.add(\cf3 'NORP'\cf0 , patterns=ethnic_adjective_patterns)\
\
\cf4 # Add patterns for ORG entities\cf0 \
org_pattern = [\{\cf3 "LOWER"\cf0 : \{\cf3 "IN"\cf0 : [\cf3 "nkvd"\cf0 , \cf3 "wehrmacht"\cf0 , \cf3 "gestapo"\cf0 , \cf3 "kgb"\cf0 ]\}\}]\
matcher.add(\cf3 'ORG'\cf0 , patterns=[org_pattern])\
\
\cf4 # Add patterns for PERSON entities\cf0 \
person_pattern = [\{\cf3 "LOWER"\cf0 : \{\cf3 "IN"\cf0 : [\cf3 "survivors"\cf0 , \cf3 "putin"\cf0 , \cf3 "hitler"\cf0 , \cf3 "stalin"\cf0 , \cf3 "forced labourer"\cf0 , \cf3 "forced labourers"\cf0 , \cf3 "forced labor"\cf0 , \cf3 "forced laborer"\cf0 ]\}\}]\
matcher.add(\cf3 'PERSON'\cf0 , patterns=[person_pattern])\
\
\cf4 # Define the set of words that should not be extracted as a person\cf0 \
not_persons = \cf8 set\cf0 ([\cf3 "maidan"\cf0 , \cf3 "germany"\cf0 , \cf3 "austria"\cf0 , \cf3 "estonia"\cf0 , \cf3 "march"\cf0 , \cf3 "buchenwald"\cf0 , \cf3 "donbas"\cf0 , \cf3 "kamieniec podolski"\cf0 , \cf3 "babi yar"\cf0 , \cf3 "ukraine germany"\cf0 , \cf3 "slava ukraine"\cf0 , \cf3 "standwithukraine"\cf0 , \cf3 "ukraine"\cf0 , \cf3 "belarus"\cf0 , \cf3 "poland"\cf0 , \cf3 "holodomor"\cf0 ])\
\
\cf4 # Function to perform entity recognition and save results to DataFrame\cf0 \
\pard\pardeftab708\ri-6\sl285\partightenfactor0
\cf5 def\cf0  \cf6 perform_entity_recognition\cf0 (\cf7 df\cf0 , \cf7 nlp\cf0 ):\
    \cf4 # Initialize lists to store results\cf0 \
    dates = []\
    events = []\
    persons = []\
    norps = []  \cf4 # Nationalities or religious or political groups\cf0 \
    orgs = []\
    new_ids = []\
\
    \cf4 # Iterate over each row in the dataframe\cf0 \
    \cf2 for\cf0  index, row \cf5 in\cf0  df.iterrows():\
        \cf4 # Check if the text is missing, and if so, skip this row\cf0 \
        \cf2 if\cf0  pd.isnull(row[\cf3 'clean_text'\cf0 ]):\
            dates.append(\cf5 None\cf0 )\
            events.append(\cf5 None\cf0 )\
            persons.append(\cf5 None\cf0 )\
            norps.append(\cf5 None\cf0 )\
            orgs.append(\cf5 None\cf0 )\
            new_ids.append(row[\cf3 'new_id'\cf0 ])\
            \cf2 continue\cf0 \
\
        \cf4 # Perform entity recognition on the text\cf0 \
        doc = nlp(row[\cf3 'clean_text'\cf0 ])\
\
        \cf4 # Extract entities of interest (DATE, EVENT, PERSON, NORP, ORG)\cf0 \
        extracted_dates = []\
        extracted_events = []\
        extracted_persons = []\
        extracted_norps = []\
        extracted_orgs = []\
\
        \cf2 for\cf0  ent \cf5 in\cf0  doc.ents:\
            \cf2 if\cf0  ent.label_ == \cf3 'DATE'\cf0 :\
                extracted_dates.append(ent.text)\
            \cf2 elif\cf0  ent.label_ == \cf3 'EVENT'\cf0 :\
                extracted_events.append(ent.text)\
            \cf2 elif\cf0  ent.label_ == \cf3 'PERSON'\cf0 :\
                extracted_persons.append(ent.text)\
            \cf2 elif\cf0  ent.label_ == \cf3 'NORP'\cf0 :\
                extracted_norps.append(ent.text)\
            \cf2 elif\cf0  ent.label_ == \cf3 'ORG'\cf0 :\
                extracted_orgs.append(ent.text)\
\
\pard\pardeftab708\ri-6\sl285\partightenfactor0
\cf4 # Apply the custom matcher\cf0 \
        matches = matcher(doc)\
        \cf2 for\cf0  match_id, start, end \cf5 in\cf0  matches:\
            span = doc[start:end]\
            match_label = nlp.vocab.strings[match_id]\
            \cf2 if\cf0  match_label == \cf3 'DATE'\cf0 :\
                extracted_dates.append(span.text)\
            \cf2 elif\cf0  match_label == \cf3 'EVENT'\cf0 :\
                extracted_events.append(span.text)\
            \cf2 elif\cf0  match_label == \cf3 'NORP'\cf0 :\
                extracted_norps.append(span.text)\
            \cf2 elif\cf0  match_label == \cf3 'ORG'\cf0 :\
                extracted_orgs.append(span.text)\
            \cf2 elif\cf0  match_label == \cf3 'PERSON'\cf0 :\
                \cf2 if\cf0  span.text.lower() \cf5 not\cf0  \cf5 in\cf0  not_persons:\
                    extracted_persons.append(span.text)\
\pard\pardeftab708\ri-6\sl285\sa240\partightenfactor0
\cf0 \
\pard\pardeftab708\ri-6\sl285\partightenfactor0
\cf0         \cf4 # Append the extracted entities to the respective lists\cf0 \
        dates.append(extracted_dates)\
        events.append(extracted_events)\
        persons.append(extracted_persons)\
        norps.append(extracted_norps)\
        orgs.append(extracted_orgs)\
\
        \cf4 # Append other relevant information\cf0 \
        new_ids.append(row[\cf3 'new_id'\cf0 ])\
\
    \cf4 # Create a new dataframe to store the results\cf0 \
    result_df = pd.DataFrame(\{\
        \cf3 'new_id'\cf0 : new_ids,\
        \cf3 'date'\cf0 : dates,\
        \cf3 'event'\cf0 : events,\
        \cf3 'person'\cf0 : persons,\
        \cf3 'norp'\cf0 : norps,\
        \cf3 'org'\cf0 : orgs\
    \})\
\
    \cf2 return\cf0  result_df\
\
\pard\pardeftab708\ri-6\sl285\partightenfactor0
\cf4 # Call the function to perform entity recognition for the English language model\cf0 \
result_df = perform_entity_recognition(df, nlp_en)\
\
\cf4 # Save the dataframe to Excel\cf0 \
result_df.to_excel(\cf3 '/x.xlsx'\cf0 , index=\cf5 False\cf0 )\
\
\cf4 # Download the Excel file\cf0 \
\pard\pardeftab708\ri-6\sl285\partightenfactor0
\cf2 from\cf0  google.colab \cf2 import\cf0  files\
files.download(\cf3 '/x.xlsx'\cf0 )\
\pard\pardeftab708\ri-6\partightenfactor0

\f0\fs24 \cf0 \kerning1\expnd0\expndtw0 \
}